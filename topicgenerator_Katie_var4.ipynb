{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import sys\n",
    "from operator import add\n",
    "from pyspark import SparkContext\n",
    "import re\n",
    "from wordfreq import word_frequency\n",
    "import collections\n",
    "from gensim.models import Word2Vec\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "\n",
    "import unidecode\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outfile = open('data.txt', 'w')\n",
    "\n",
    "with open('satisfaction.csv') as csvDataFile:\n",
    "    csvReader = csv.reader(csvDataFile)\n",
    "    for row in csvReader:\n",
    "        outfile.write(\"%s\\n\" % row[2])\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def read_words(words_file):\n",
    "    return [word.lower() for line in open(words_file, 'r') for word in tokenizer.tokenize(line)]\n",
    "\n",
    "words = read_words('data.txt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('sanitized')\n",
    "filtered_words = [w for w in words if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14167\n",
      "7601\n"
     ]
    }
   ],
   "source": [
    "print(len(words))\n",
    "print(len(filtered_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Frequent Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('easy', 'use'), ('customer', 'service'), ('ease', 'use'), ('user', 'friendly'), ('tech', 'support'), ('question', 'types'), ('customer', 'support'), ('great', 'customer'), ('survey', 'tool'), ('great', 'tool'), ('easy', 'learn'), ('survey', 'monkey'), ('already', 'recommended'), ('row', 'per'), ('e', 'g'), ('top', 'notch'), ('quality', 'product'), ('panel', 'triggers'), ('much', 'easier'), ('reasonably', 'priced')]\n"
     ]
    }
   ],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(filtered_words, window_size=2)\n",
    "finder.apply_freq_filter(1)\n",
    "top_bigrams = finder.nbest(bigram_measures.likelihood_ratio, 20)\n",
    "print(top_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_words = [w for w in words if not w in stop_words]\n",
    "combined_words = filtered_words\n",
    "numB = len(filtered_words) - 1\n",
    "count = 0\n",
    "for i in range(numB):\n",
    "    bigram = (filtered_words[i], filtered_words[i+1])\n",
    "    if bigram in top_bigrams:\n",
    "        j = i - count\n",
    "        k = (i + 1) - count\n",
    "        combined_words[j:k] = ['_'.join(bigram)]\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reasons',\n",
       " 'score',\n",
       " 'excellent',\n",
       " 'platform',\n",
       " 'highly',\n",
       " 'responsive',\n",
       " 'customer_service',\n",
       " 'service',\n",
       " 'variety',\n",
       " 'questions']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIP Scores\n",
    "An alternative method that is a bit faster for smaller datasets than using Spark. Note that dividing by \"num_words\" normalizes the SIP score, so that any word with SIP = 1 is used in the corpus just as often as in normal english language. A word with SIP = 5 is used 5 times as often in the corpus as it is in normal english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_words = len(combined_words)\n",
    "text = nltk.Text(combined_words)\n",
    "uniq_words = list(set(combined_words))\n",
    "word_pool = []\n",
    "word_count = []\n",
    "word_SIP = []\n",
    "for word in uniq_words:\n",
    "    c = text.count(word)\n",
    "    # only interested in words that occur more than 5 times\n",
    "    if c < 6:\n",
    "        continue\n",
    "    word_pool.append(word)\n",
    "    freq = word_frequency(word, 'en')\n",
    "    if freq == 0:\n",
    "        freq = float(.00001)\n",
    "    # normalize the SIP scores\n",
    "    word_SIP.append(c/num_words/freq)\n",
    "    word_count.append(c)\n",
    "\n",
    "word_pool_sorted = [x for _,x in sorted(zip(word_SIP, word_pool), reverse = True)]\n",
    "word_count_sorted = [x for _,x in sorted(zip(word_SIP, word_count), reverse = True)]\n",
    "word_SIP_sorted = word_SIP\n",
    "word_SIP_sorted.sort(reverse = True)\n",
    "SIPscoreslist = list(zip(word_pool_sorted, word_SIP_sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('qualtrics', 1881.3314037626626),\n",
       " ('easy_use', 1499.8026575450597),\n",
       " ('customer_service', 1052.493093014077),\n",
       " ('surveys', 848.5417258661168),\n",
       " ('intuitive', 765.7775240465305),\n",
       " ('functionality', 731.609347717143),\n",
       " ('ease_use', 710.432837784502),\n",
       " ('user_friendly', 447.30956453098275),\n",
       " ('responsive', 393.74996348550434),\n",
       " ('customer', 365.2187140363602),\n",
       " ('ease', 352.59854662620455),\n",
       " ('tool', 333.7119639702683),\n",
       " ('survey', 318.09068592290186),\n",
       " ('triggers', 304.3489283636335),\n",
       " ('customer_support', 302.59176424154714),\n",
       " ('survey_tool', 236.81094592816734),\n",
       " ('great_tool', 236.81094592816734),\n",
       " ('robust', 225.46161822435596),\n",
       " ('capabilities', 219.45655741626007),\n",
       " ('tech_support', 184.18629127746348)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SIPscoreslist[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "Word2Vec feels very unstable to me, at least with this few responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [combined_words]\n",
    "model = Word2Vec(sentences, size=300, window=4, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('support', 0.9393998980522156), ('service', 0.9318608641624451), ('survey', 0.9308834671974182), ('easy', 0.9308336973190308), ('use', 0.9290825724601746)]\n",
      "[('customer_service', 0.924868106842041), ('service', 0.9244619607925415), ('support', 0.9243979454040527), ('qualtrics', 0.9235543608665466), ('use', 0.9222701191902161)]\n",
      "[('use', 0.9071529507637024), ('qualtrics', 0.9061402082443237), ('customer_service', 0.9014229774475098), ('survey', 0.9013200998306274), ('easy', 0.8990861177444458)]\n"
     ]
    }
   ],
   "source": [
    "# most similar words according to Word2Vec\n",
    "print(model.wv.most_similar(positive=['qualtrics'], topn=5))\n",
    "print(model.wv.most_similar(positive=['easy_use'], topn=5))\n",
    "print(model.wv.most_similar(positive=['tool'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.860642877298\n",
      "0.922574525181\n",
      "0.290757756691\n",
      "0.502701048523\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('customer_support', 'customer_service'))\n",
    "print(model.wv.similarity('customer', 'support'))\n",
    "print(model.wv.similarity('affordable', 'cost'))\n",
    "print(model.wv.similarity('cost','price'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonymns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this takes longer...so let's limit to top 30\n",
    "NUM_TOPICS = 30\n",
    "top_words = word_pool_sorted[:NUM_TOPICS]\n",
    "synonyms = []\n",
    "for word in top_words:\n",
    "    word_synonyms = []\n",
    "    for syn in wn.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            word_synonyms.append(l.name())\n",
    "    synonyms.append(list(set(word_synonyms[:3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [],\n",
       " ['study', 'sketch', 'survey'],\n",
       " ['intuitive', 'visceral', 'nonrational'],\n",
       " [],\n",
       " ['functionality'],\n",
       " [],\n",
       " ['antiphonal', 'responsive', 'reactive'],\n",
       " ['customer', 'client'],\n",
       " ['ease', 'simplicity', 'simpleness', 'easiness'],\n",
       " ['instrument', 'tool', 'creature'],\n",
       " ['study', 'sketch', 'survey'],\n",
       " ['gun_trigger', 'trigger'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['capability', 'capacity', 'capableness'],\n",
       " ['rich', 'robust', 'racy', 'full-bodied'],\n",
       " [],\n",
       " [],\n",
       " ['elastic', 'flexile', 'flexible'],\n",
       " ['reportage', 'coverage', 'reporting', 'report'],\n",
       " ['flexibility', 'flexibleness'],\n",
       " ['user_interface', 'interface'],\n",
       " ['favorable', 'friendly'],\n",
       " ['stand_out', 'excel', 'surpass'],\n",
       " ['ware', 'product', 'merchandise'],\n",
       " ['political_platform', 'platform', 'political_program'],\n",
       " []]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base \"Noun\" Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "clusters = []\n",
    "blacklist = {}\n",
    "for i, parent_word in enumerate(SIPscoreslist):\n",
    "    if parent_word[0] not in blacklist:\n",
    "        # this is trying to account for \"survey\" and \"surveys\"\n",
    "        if stemmer.stem(parent_word[0]) not in blacklist:\n",
    "            \n",
    "            # begin the cluster with parent_word\n",
    "            cluster = []\n",
    "            blacklist[parent_word[0]] = 1\n",
    "            cluster.append(parent_word[0])\n",
    "            \n",
    "            # add 6 similar words from Word2Vec\n",
    "            top_sim = [p[0] for p in model.wv.most_similar(positive=parent_word[0], topn=20)] # top 20\n",
    "            top_sim = [w for w in top_sim if w not in blacklist] # remove any in blacklist\n",
    "            top_sim = [w for w in top_sim if stemmer.stem(w) not in blacklist]\n",
    "            for word in top_sim[:6]: # take the top 6 not in blacklist\n",
    "                cluster.append(word)\n",
    "                blacklist[word] = 1\n",
    "            \n",
    "            # add 3 similar words from WordNet synonyms\n",
    "            top_sim = synonyms[i]\n",
    "            for word in top_sim:\n",
    "                if word not in blacklist:\n",
    "                    if stemmer.stem(word) not in blacklist:\n",
    "                        cluster.append(word)\n",
    "                        blacklist[word] = 1\n",
    "            \n",
    "            # finish the cluster\n",
    "            clusters.append(cluster)\n",
    "    # force it to stop if we run past NUM_TOPICS\n",
    "    if i == NUM_TOPICS - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(model.wv.most_similar(positive=['qualtrics'], topn=10))\n",
    "# print(model.wv.most_similar(positive=['user_friendly'], topn=10))\n",
    "# print(model.wv.most_similar(positive=['great_tool'], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "['qualtrics', 'support', 'service', 'survey', 'easy', 'use', 'customer_service']\n",
      "['easy_use', 'features', 'great', 'customer', 'product', 'like', 'software']\n",
      "['intuitive', 'tool', 'time', 'good', 'ease_use', 'reporting', 'nonrational']\n",
      "['functionality', 'need', 'work', 'get']\n",
      "['robust', 'help', 'racy', 'full-bodied']\n"
     ]
    }
   ],
   "source": [
    "print(len(clusters))\n",
    "print(clusters[0])\n",
    "print(clusters[1])\n",
    "print(clusters[2])\n",
    "print(clusters[3])\n",
    "print(clusters[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive \"Adjective\" Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "descriptions = []\n",
    "\n",
    "# this is a bigram filter to remove bigrams with words in the blacklist\n",
    "def create_myfilter(parent):\n",
    "    def bigram_filter(w1, w2):\n",
    "        if w1 == parent:\n",
    "            return w2 in blacklist\n",
    "        if w2 == parent:\n",
    "            return w1 in blacklist\n",
    "    return bigram_filter\n",
    "\n",
    "# for every main/noun cluster....\n",
    "for i, cluster in enumerate(clusters):\n",
    "    # make a copy of the text, and replace \n",
    "    # all cluster words with the parent_word of that cluster\n",
    "    parent_word = cluster[0]\n",
    "    similar_words = cluster[1:]\n",
    "    words_copy = [parent_word if w in similar_words else w for w in combined_words]\n",
    "    \n",
    "    # find top 10 bigrams containing the parent_word\n",
    "    # and NOT contains blacklist words\n",
    "    finder = BigramCollocationFinder.from_words(words_copy, window_size=3)\n",
    "    parent_filter = lambda *w: parent_word not in w   \n",
    "    blacklist_filter = create_myfilter(parent_word)\n",
    "    finder.apply_ngram_filter(parent_filter)      # bigram must contain parent_word\n",
    "    finder.apply_ngram_filter(blacklist_filter)   # bigram does not contain blacklist words\n",
    "    finder.apply_freq_filter(3)                   # bigram occurs at least 3 times\n",
    "    best_bigrams = finder.nbest(bigram_measures.likelihood_ratio, 10)\n",
    "    adj = []\n",
    "    list(adj.extend(row) for row in best_bigrams)\n",
    "    l = [w for w in adj if w != parent_word]\n",
    "    descriptions.append(adj)\n",
    "\n",
    "for i,cluster in enumerate(clusters):\n",
    "    descriptions[i] = [w for w in descriptions[i] if w != cluster[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['affordable', 'tech', 'always', 'learn', 'versatile', 'fairly', 'monkey', 'allows', 'let', 'staff']\n",
      "['quality', 'adding', 'layout', 'documentation', 'centric', 'collaboration', 'efficient', 'overall', 'wish', 'pleased']\n",
      "['functions', 'still', 'money', 'surveys', '360', 'outstanding', 'tools', 'lot', 'research', 'improvement']\n"
     ]
    }
   ],
   "source": [
    "print(descriptions[0])\n",
    "print(descriptions[1])\n",
    "print(descriptions[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def create_clustfilter(clust):\n",
    "#     def bigram_filter(w1, w2):\n",
    "#         return w2 not in clust and w1 not in clust\n",
    "#     return bigram_filter\n",
    "\n",
    "# topics = []\n",
    "# for i in range(len(clusters)):\n",
    "#     pool = clusters[i] + descriptions[i]\n",
    "#     finder = BigramCollocationFinder.from_words(combined_words, window_size=2)\n",
    "#     clust_filter = create_clustfilter(pool)  \n",
    "#     topic_filter = lambda w1, w2: (w1, w2) in set(topics)\n",
    "#     finder.apply_ngram_filter(clust_filter)\n",
    "#     finder.apply_ngram_filter(topic_filter)\n",
    "#     topics.append(finder.nbest(bigram_measures.likelihood_ratio, 1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_topics = []\n",
    "for i,clust in enumerate(clusters):\n",
    "    adj = descriptions[i]\n",
    "    if len(adj) == 0:\n",
    "        topicName = clust[0]\n",
    "        queryParams = [topicName, clust, []]\n",
    "    else:\n",
    "        topicName = \" \".join([clust[0], adj[0]])\n",
    "        queryParams = [topicName, clust, adj]\n",
    "    final_topics.append(queryParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['qualtrics affordable',\n",
       "  ['qualtrics',\n",
       "   'support',\n",
       "   'service',\n",
       "   'survey',\n",
       "   'easy',\n",
       "   'use',\n",
       "   'customer_service'],\n",
       "  ['affordable',\n",
       "   'tech',\n",
       "   'always',\n",
       "   'learn',\n",
       "   'versatile',\n",
       "   'fairly',\n",
       "   'monkey',\n",
       "   'allows',\n",
       "   'let',\n",
       "   'staff']],\n",
       " ['easy_use quality',\n",
       "  ['easy_use', 'features', 'great', 'customer', 'product', 'like', 'software'],\n",
       "  ['quality',\n",
       "   'adding',\n",
       "   'layout',\n",
       "   'documentation',\n",
       "   'centric',\n",
       "   'collaboration',\n",
       "   'efficient',\n",
       "   'overall',\n",
       "   'wish',\n",
       "   'pleased']],\n",
       " ['intuitive functions',\n",
       "  ['intuitive',\n",
       "   'tool',\n",
       "   'time',\n",
       "   'good',\n",
       "   'ease_use',\n",
       "   'reporting',\n",
       "   'nonrational'],\n",
       "  ['functions',\n",
       "   'still',\n",
       "   'money',\n",
       "   'surveys',\n",
       "   '360',\n",
       "   'outstanding',\n",
       "   'tools',\n",
       "   'lot',\n",
       "   'research',\n",
       "   'improvement']]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_topics[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outfile = open('topicsKaite_var4.txt', 'w')\n",
    "\n",
    "for topic in final_topics[:10]:\n",
    "    outfile.write(\"%s\\n\" % topic[0])\n",
    "    outfile.write(\"%s\\n\" % topic[1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
