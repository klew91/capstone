{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import sys\n",
    "from operator import add\n",
    "from pyspark import SparkContext\n",
    "import re\n",
    "from wordfreq import word_frequency\n",
    "import collections\n",
    "from gensim.models import Word2Vec\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "\n",
    "import unidecode\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outfile = open('data.txt', 'w')\n",
    "\n",
    "with open('satisfaction.csv') as csvDataFile:\n",
    "    csvReader = csv.reader(csvDataFile)\n",
    "    for row in csvReader:\n",
    "        outfile.write(\"%s\\n\" % row[2])\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def read_words(words_file):\n",
    "    return [word.lower() for line in open(words_file, 'r') for word in tokenizer.tokenize(line)]\n",
    "\n",
    "words = read_words('data.txt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('sanitized')\n",
    "filtered_words = [w for w in words if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14167\n",
      "7601\n"
     ]
    }
   ],
   "source": [
    "print(len(words))\n",
    "print(len(filtered_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Frequent Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('easy', 'use'), ('customer', 'service'), ('ease', 'use'), ('user', 'friendly'), ('tech', 'support'), ('question', 'types'), ('customer', 'support'), ('great', 'customer'), ('survey', 'tool'), ('great', 'tool'), ('easy', 'learn'), ('survey', 'monkey'), ('already', 'recommended'), ('row', 'per'), ('quality', 'product'), ('panel', 'triggers'), ('much', 'easier'), ('one', 'row'), ('table', 'one'), ('great', 'product')]\n"
     ]
    }
   ],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(filtered_words, window_size=2)\n",
    "finder.apply_freq_filter(4)\n",
    "top_bigrams = finder.nbest(bigram_measures.likelihood_ratio, 20)\n",
    "print(top_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_words = [w for w in words if not w in stop_words]\n",
    "combined_words = filtered_words\n",
    "numB = len(filtered_words) - 1\n",
    "count = 0\n",
    "for i in range(numB):\n",
    "    bigram = (filtered_words[i], filtered_words[i+1])\n",
    "    if bigram in top_bigrams:\n",
    "        j = i - count\n",
    "        k = (i + 1) - count\n",
    "        combined_words[j:k] = ['_'.join(bigram)]\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reasons',\n",
       " 'score',\n",
       " 'excellent',\n",
       " 'platform',\n",
       " 'highly',\n",
       " 'responsive',\n",
       " 'customer_service',\n",
       " 'service',\n",
       " 'variety',\n",
       " 'questions']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIP Scores\n",
    "An alternative method that is a bit faster for smaller datasets than using Spark. Note that dividing by \"num_words\" normalizes the SIP score, so that any word with SIP = 1 is used in the corpus just as often as in normal english language. A word with SIP = 5 is used 5 times as often in the corpus as it is in normal english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_words = len(combined_words)\n",
    "text = nltk.Text(combined_words)\n",
    "uniq_words = list(set(combined_words))\n",
    "word_pool = []\n",
    "word_count = []\n",
    "word_SIP = []\n",
    "for word in uniq_words:\n",
    "    c = text.count(word)\n",
    "    # only interested in words that occur more than 10 times\n",
    "    if c < 11:\n",
    "        continue\n",
    "    word_pool.append(word)\n",
    "    freq = word_frequency(word, 'en')\n",
    "    if freq == 0:\n",
    "        freq = float(.00001)\n",
    "    # normalize the SIP scores\n",
    "    word_SIP.append(c/num_words/freq)\n",
    "    word_count.append(c)\n",
    "\n",
    "word_pool_sorted = [x for _,x in sorted(zip(word_SIP, word_pool), reverse = True)]\n",
    "word_count_sorted = [x for _,x in sorted(zip(word_SIP, word_count), reverse = True)]\n",
    "word_SIP_sorted = word_SIP\n",
    "word_SIP_sorted.sort(reverse = True)\n",
    "SIPscoreslist = list(zip(word_pool_sorted, word_SIP_sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('qualtrics', 1881.3314037626626),\n",
       " ('easy_use', 1499.8026575450597),\n",
       " ('customer_service', 1052.493093014077),\n",
       " ('surveys', 861.3984186822701),\n",
       " ('intuitive', 736.3245423524331),\n",
       " ('ease_use', 710.432837784502),\n",
       " ('functionality', 708.7465556009823),\n",
       " ('user_friendly', 447.30956453098275),\n",
       " ('responsive', 367.49996591980397),\n",
       " ('customer', 365.2187140363602),\n",
       " ('ease', 346.72190418243446),\n",
       " ('tool', 330.00405325948753),\n",
       " ('survey', 320.65593339002197),\n",
       " ('customer_support', 302.59176424154714),\n",
       " ('survey_tool', 236.81094592816734)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SIPscoreslist[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "Word2Vec feels very unstable to me, at least with this few responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [combined_words]\n",
    "model = Word2Vec(sentences, size=300, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('use', 0.9681816697120667), ('easy', 0.9637120962142944), ('easy_use', 0.9594351053237915), ('survey', 0.9586905241012573), ('service', 0.9581952691078186)]\n",
      "[('qualtrics', 0.9594351053237915), ('use', 0.9579805135726929), ('easy', 0.9527338147163391), ('support', 0.9501928091049194), ('service', 0.9495858550071716)]\n",
      "[('use', 0.9490972757339478), ('easy', 0.9471516013145447), ('qualtrics', 0.9438937306404114), ('great', 0.9418753385543823), ('support', 0.9384625554084778)]\n"
     ]
    }
   ],
   "source": [
    "# most similar words according to Word2Vec\n",
    "print(model.wv.most_similar(positive=['qualtrics'], topn=5))\n",
    "print(model.wv.most_similar(positive=['easy_use'], topn=5))\n",
    "print(model.wv.most_similar(positive=['tool'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.853489343449\n",
      "0.930141041698\n",
      "0.430844719892\n",
      "0.587260808308\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('customer_support', 'customer_service'))\n",
    "print(model.wv.similarity('customer', 'support'))\n",
    "print(model.wv.similarity('affordable', 'cost'))\n",
    "print(model.wv.similarity('cost','price'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonymns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this takes longer...so let's limit to top 30\n",
    "NUM_TOPICS = 30\n",
    "top_words = word_pool_sorted[:NUM_TOPICS]\n",
    "synonyms = []\n",
    "for word in top_words:\n",
    "    word_synonyms = []\n",
    "    for syn in wn.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            word_synonyms.append(l.name())\n",
    "    synonyms.append(list(set(word_synonyms[:4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [],\n",
       " ['survey', 'study', 'sketch'],\n",
       " ['intuitive', 'visceral', 'nonrational'],\n",
       " [],\n",
       " ['functionality'],\n",
       " [],\n",
       " ['antiphonal', 'reactive', 'responsive'],\n",
       " ['customer', 'client'],\n",
       " ['easiness', 'simplicity', 'simpleness', 'ease'],\n",
       " ['instrument', 'tool', 'creature'],\n",
       " ['survey', 'study', 'sketch'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['capability', 'capableness', 'capacity'],\n",
       " ['rich', 'racy', 'robust', 'full-bodied'],\n",
       " [],\n",
       " [],\n",
       " ['flexible', 'flexile', 'elastic'],\n",
       " ['coverage', 'report', 'reporting', 'reportage'],\n",
       " ['flexibleness', 'flexibility'],\n",
       " ['interface', 'user_interface'],\n",
       " ['favorable', 'friendly'],\n",
       " ['merchandise', 'ware', 'product'],\n",
       " ['political_platform', 'political_program', 'platform'],\n",
       " ['first-class', 'splendid', 'excellent', 'fantabulous'],\n",
       " ['easy', 'easygoing', 'leisurely'],\n",
       " ['user', 'drug_user', 'exploiter']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base \"Noun\" Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "clusters = []\n",
    "blacklist = {}\n",
    "for i, parent_word in enumerate(SIPscoreslist):\n",
    "    if parent_word[0] not in blacklist:\n",
    "        # this is trying to account for \"survey\" and \"surveys\"\n",
    "        if stemmer.stem(parent_word[0]) not in blacklist:\n",
    "            \n",
    "            # begin the cluster with parent_word\n",
    "            cluster = []\n",
    "            blacklist[parent_word[0]] = 1\n",
    "            cluster.append(parent_word[0])\n",
    "            \n",
    "            # add 5 similar words from Word2Vec\n",
    "            top_sim = [p[0] for p in model.wv.most_similar(positive=parent_word[0], topn=20)] # top 20\n",
    "            top_sim = [w for w in top_sim if w not in blacklist] # remove any in blacklist\n",
    "            top_sim = [w for w in top_sim if stemmer.stem(w) not in blacklist]\n",
    "            for word in top_sim[:5]: # take the top 5 not in blacklist\n",
    "                cluster.append(word)\n",
    "                blacklist[word] = 1\n",
    "            \n",
    "            # add 4 similar words from WordNet synonyms\n",
    "            top_sim = synonyms[i]\n",
    "            for word in top_sim:\n",
    "                if word not in blacklist:\n",
    "                    if stemmer.stem(word) not in blacklist:\n",
    "                        cluster.append(word)\n",
    "                        blacklist[word] = 1\n",
    "            \n",
    "            # finish the cluster\n",
    "            clusters.append(cluster)\n",
    "    # force it to stop if we run past NUM_TOPICS\n",
    "    if i == NUM_TOPICS - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(model.wv.most_similar(positive=['qualtrics'], topn=10))\n",
    "# print(model.wv.most_similar(positive=['user_friendly'], topn=10))\n",
    "# print(model.wv.most_similar(positive=['great_tool'], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "['qualtrics', 'use', 'easy', 'easy_use', 'survey', 'service']\n",
      "['customer_service', 'customer', 'support', 'good', 'great', 'product']\n",
      "['intuitive', 'like', 'friendly', 'ease', 'tool', 'user', 'visceral', 'nonrational']\n",
      "['ease_use', 'features', 'time', 'data']\n",
      "['capabilities', 'much', 'capability', 'capableness', 'capacity']\n"
     ]
    }
   ],
   "source": [
    "print(len(clusters))\n",
    "print(clusters[0])\n",
    "print(clusters[1])\n",
    "print(clusters[2])\n",
    "print(clusters[3])\n",
    "print(clusters[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive \"Adjective\" Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "descriptions = []\n",
    "\n",
    "# this is a bigram filter to remove bigrams with words in the blacklist\n",
    "def create_myfilter(parent):\n",
    "    def bigram_filter(w1, w2):\n",
    "        if w1 == parent:\n",
    "            return w2 in blacklist\n",
    "        if w2 == parent:\n",
    "            return w1 in blacklist\n",
    "    return bigram_filter\n",
    "\n",
    "# for every main/noun cluster....\n",
    "for i, cluster in enumerate(clusters):\n",
    "    # make a copy of the text, and replace \n",
    "    # all cluster words with the parent_word of that cluster\n",
    "    parent_word = cluster[0]\n",
    "    similar_words = cluster[1:]\n",
    "    words_copy = [parent_word if w in similar_words else w for w in combined_words]\n",
    "    \n",
    "    # find top 10 bigrams containing the parent_word\n",
    "    # and NOT contains blacklist words\n",
    "    finder = BigramCollocationFinder.from_words(words_copy, window_size=5)\n",
    "    parent_filter = lambda *w: parent_word not in w   \n",
    "    blacklist_filter = create_myfilter(parent_word)\n",
    "    finder.apply_ngram_filter(parent_filter)      # bigram must contain parent_word\n",
    "    finder.apply_ngram_filter(blacklist_filter)   # bigram does not contain blacklist words\n",
    "    finder.apply_freq_filter(3)                   # bigram occurs at least 3 times\n",
    "    best_bigrams = finder.nbest(bigram_measures.likelihood_ratio, 10)\n",
    "    adj = []\n",
    "    list(adj.extend(row) for row in best_bigrams)\n",
    "    l = [w for w in adj if w != parent_word]\n",
    "    descriptions.append(adj)\n",
    "\n",
    "for i,cluster in enumerate(clusters):\n",
    "    descriptions[i] = [w for w in descriptions[i] if w != cluster[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lot', 'affordable', 'enables', 'versitile', 'versatile', 'fairly', 'wide', 'sophisticated', 'relatively', 'provides']\n",
      "['accessibility', 'restrictive', 'monkey', 'row_per', 'package', 'sent', 'layout', 'purpose', 'improvements', 'centric']\n",
      "['desk', 'organization', 'value', 'functional', 'bit', 'pretty', 'really', 'satisfied', 'quality', '8']\n"
     ]
    }
   ],
   "source": [
    "print(descriptions[0])\n",
    "print(descriptions[1])\n",
    "print(descriptions[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_clustfilter(clust):\n",
    "#     def bigram_filter(w1, w2):\n",
    "#         return w2 not in clust or w1 not in clust\n",
    "#     return bigram_filter\n",
    "\n",
    "# topics = []\n",
    "# for i in range(len(clusters)):\n",
    "#     pool = clusters[i] + descriptions[i]\n",
    "#     finder = BigramCollocationFinder.from_words(combined_words, window_size=2)\n",
    "#     clust_filter = create_clustfilter(pool)  \n",
    "#     topic_filter = lambda w1, w2: (w1, w2) in set(topics)\n",
    "#     finder.apply_ngram_filter(clust_filter)\n",
    "#     finder.apply_ngram_filter(topic_filter)\n",
    "#     topics.append(finder.nbest(bigram_measures.likelihood_ratio, 1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_topics = []\n",
    "for i,clust in enumerate(clusters):\n",
    "    adj = descriptions[i]\n",
    "    if len(adj) == 0:\n",
    "        topicName = clust[0]\n",
    "        queryParams = [topicName, clust, []]\n",
    "    else:\n",
    "        topicName = \" \".join([clust[0], adj[0]])\n",
    "        queryParams = [topicName, clust, adj]\n",
    "    final_topics.append(queryParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['qualtrics lot',\n",
       "  ['qualtrics', 'use', 'easy_use', 'easy', 'support', 'service'],\n",
       "  ['lot',\n",
       "   'affordable',\n",
       "   'enables',\n",
       "   'versitile',\n",
       "   'versatile',\n",
       "   'fairly',\n",
       "   'wide',\n",
       "   'sophisticated',\n",
       "   'relatively',\n",
       "   'provides']],\n",
       " ['customer_service accessibility',\n",
       "  ['customer_service', 'survey', 'customer', 'good', 'great', 'user'],\n",
       "  ['accessibility',\n",
       "   'restrictive',\n",
       "   'monkey',\n",
       "   'row_per',\n",
       "   'package',\n",
       "   'sent',\n",
       "   'layout',\n",
       "   'purpose',\n",
       "   'improvements',\n",
       "   'centric']]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_topics[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outfile = open('topicsKaite_var2.txt', 'w')\n",
    "\n",
    "for topic in final_topics[:10]:\n",
    "    outfile.write(\"%s\\n\" % topic[0])\n",
    "    outfile.write(\"%s\\n\" % topic[1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
