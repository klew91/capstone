{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import sys\n",
    "from operator import add\n",
    "from pyspark import SparkContext\n",
    "import re\n",
    "from wordfreq import word_frequency\n",
    "import collections\n",
    "from gensim.models import Word2Vec\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "\n",
    "import unidecode\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outfile = open('data.txt', 'w')\n",
    "\n",
    "with open('satisfaction.csv') as csvDataFile:\n",
    "    csvReader = csv.reader(csvDataFile)\n",
    "    for row in csvReader:\n",
    "        outfile.write(\"%s\\n\" % row[2])\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def read_words(words_file):\n",
    "    return [word.lower() for line in open(words_file, 'r') for word in tokenizer.tokenize(line)]\n",
    "\n",
    "words = read_words('data.txt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('sanitized')\n",
    "filtered_words = [w for w in words if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14167\n",
      "7601\n"
     ]
    }
   ],
   "source": [
    "print(len(words))\n",
    "print(len(filtered_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Frequent Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(filtered_words, window_size=2)\n",
    "finder.apply_freq_filter(3)\n",
    "top_bigrams = finder.nbest(bigram_measures.likelihood_ratio, 20)\n",
    "# print(top_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_words = [w for w in words if not w in stop_words]\n",
    "combined_words = filtered_words\n",
    "numB = len(filtered_words) - 1\n",
    "count = 0\n",
    "for i in range(numB):\n",
    "    bigram = (filtered_words[i], filtered_words[i+1])\n",
    "    if bigram in top_bigrams:\n",
    "        j = i - count\n",
    "        k = (i + 1) - count\n",
    "        combined_words[j:k] = ['_'.join(bigram)]\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reasons',\n",
       " 'score',\n",
       " 'excellent',\n",
       " 'platform',\n",
       " 'highly',\n",
       " 'responsive',\n",
       " 'customer_service',\n",
       " 'service',\n",
       " 'variety',\n",
       " 'questions']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIP Scores\n",
    "An alternative method that is a bit faster for smaller datasets than using Spark. Note that dividing by \"num_words\" normalizes the SIP score, so that any word with SIP = 1 is used in the corpus just as often as in normal english language. A word with SIP = 5 is used 5 times as often in the corpus as it is in normal english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_words = len(combined_words)\n",
    "text = nltk.Text(combined_words)\n",
    "uniq_words = list(set(combined_words))\n",
    "word_pool = []\n",
    "word_count = []\n",
    "word_SIP = []\n",
    "for word in uniq_words:\n",
    "    c = text.count(word)\n",
    "    # only interested in words that occur more than 5 times\n",
    "    if c < 6:\n",
    "        continue\n",
    "    word_pool.append(word)\n",
    "    freq = word_frequency(word, 'en')\n",
    "    if freq == 0:\n",
    "        freq = float(.00001)\n",
    "    # normalize the SIP scores\n",
    "    word_SIP.append(c/num_words/freq)\n",
    "    word_count.append(c)\n",
    "\n",
    "word_pool_sorted = [x for _,x in sorted(zip(word_SIP, word_pool), reverse = True)]\n",
    "word_count_sorted = [x for _,x in sorted(zip(word_SIP, word_count), reverse = True)]\n",
    "word_SIP_sorted = word_SIP\n",
    "word_SIP_sorted.sort(reverse = True)\n",
    "SIPscoreslist = list(zip(word_pool_sorted, word_SIP_sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('qualtrics', 1881.3314037626626),\n",
       " ('easy_use', 1499.8026575450597),\n",
       " ('customer_service', 1052.493093014077),\n",
       " ('surveys', 848.5417258661168),\n",
       " ('intuitive', 765.7775240465305),\n",
       " ('functionality', 731.609347717143),\n",
       " ('ease_use', 710.432837784502),\n",
       " ('user_friendly', 447.30956453098275),\n",
       " ('responsive', 393.74996348550434),\n",
       " ('customer', 365.2187140363602),\n",
       " ('ease', 352.59854662620455),\n",
       " ('tool', 333.7119639702683),\n",
       " ('survey', 318.09068592290186),\n",
       " ('triggers', 304.3489283636335),\n",
       " ('customer_support', 302.59176424154714)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SIPscoreslist[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "Word2Vec feels very unstable to me, at least with this few responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [combined_words]\n",
    "model = Word2Vec(sentences, size=300, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('use', 0.6600898504257202), ('support', 0.6477744579315186), ('survey', 0.6268907785415649), ('great', 0.6185702085494995), ('service', 0.612062931060791)]\n",
      "[('survey', 0.6335833072662354), ('use', 0.6148089170455933), ('customer_service', 0.6006921529769897), ('service', 0.5964181423187256), ('support', 0.5776197910308838)]\n",
      "[('use', 0.5896115899085999), ('survey', 0.5860695838928223), ('qualtrics', 0.5759097337722778), ('support', 0.5573627948760986), ('ease_use', 0.5566418170928955)]\n"
     ]
    }
   ],
   "source": [
    "# most similar words according to Word2Vec\n",
    "print(model.wv.most_similar(positive=['qualtrics'], topn=5))\n",
    "print(model.wv.most_similar(positive=['easy_use'], topn=5))\n",
    "print(model.wv.most_similar(positive=['tool'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.484917870591\n",
      "0.60785327252\n",
      "0.0881058943444\n",
      "0.199504819403\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('customer_support', 'customer_service'))\n",
    "print(model.wv.similarity('customer', 'support'))\n",
    "print(model.wv.similarity('affordable', 'cost'))\n",
    "print(model.wv.similarity('cost','price'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stem Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "stem_words = [stemmer.stem(w) for w in word_pool_sorted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonymns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this takes longer...so let's limit to top 30\n",
    "NUM_TOPICS = 30\n",
    "top_words = word_pool_sorted[:NUM_TOPICS]\n",
    "synonyms = []\n",
    "for word in top_words:\n",
    "    word_synonyms = []\n",
    "    for syn in wn.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            word_synonyms.append(l.name())\n",
    "    synonyms.append(list(set(word_synonyms[:4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [],\n",
       " ['sketch', 'study', 'survey'],\n",
       " ['visceral', 'intuitive', 'nonrational'],\n",
       " ['functionality'],\n",
       " [],\n",
       " [],\n",
       " ['responsive', 'reactive', 'antiphonal'],\n",
       " ['client', 'customer'],\n",
       " ['easiness', 'ease', 'simplicity', 'simpleness'],\n",
       " ['tool', 'instrument', 'creature'],\n",
       " ['sketch', 'study', 'survey'],\n",
       " ['gun_trigger', 'trigger'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['full-bodied', 'robust', 'racy', 'rich'],\n",
       " ['capacity', 'capability', 'capableness'],\n",
       " [],\n",
       " ['report', 'reportage', 'coverage', 'reporting'],\n",
       " ['flexible', 'flexile', 'elastic'],\n",
       " ['flexibility', 'flexibleness'],\n",
       " ['user_interface', 'interface'],\n",
       " ['friendly', 'favorable'],\n",
       " ['stand_out', 'surpass', 'excel'],\n",
       " ['ware', 'product', 'merchandise'],\n",
       " ['political_program', 'political_platform', 'platform'],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base \"Noun\" Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters = []\n",
    "blacklist = {}\n",
    "for i, parent_word in enumerate(SIPscoreslist):\n",
    "    if parent_word[0] not in blacklist:\n",
    "        # this is trying to account for \"survey\" and \"surveys\"\n",
    "        if stemmer.stem(parent_word[0]) not in blacklist:\n",
    "            \n",
    "            # begin the cluster with parent_word\n",
    "            cluster = []\n",
    "            blacklist[parent_word[0]] = 1\n",
    "            cluster.append(parent_word[0])\n",
    "            \n",
    "            # add 5 similar words from Word2Vec\n",
    "            top_sim = [p[0] for p in model.wv.most_similar(positive=parent_word[0], topn=20)] # top 20\n",
    "            top_sim = [w for w in top_sim if w not in blacklist] # remove any in blacklist\n",
    "            top_sim = [w for w in top_sim if stemmer.stem(w) not in blacklist]\n",
    "            for word in top_sim[:5]: # take the top 5 not in blacklist\n",
    "                cluster.append(word)\n",
    "                blacklist[word] = 1\n",
    "            \n",
    "            # add 4 similar words from WordNet synonyms\n",
    "            top_sim = synonyms[i]\n",
    "            for word in top_sim:\n",
    "                if word not in blacklist:\n",
    "                    if stemmer.stem(word) not in blacklist:\n",
    "                        cluster.append(word)\n",
    "                        blacklist[word] = 1\n",
    "            \n",
    "            # finish the cluster\n",
    "            clusters.append(cluster)\n",
    "    # force it to stop if we run past NUM_TOPICS\n",
    "    if i == NUM_TOPICS - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(model.wv.most_similar(positive=['qualtrics'], topn=10))\n",
    "# print(model.wv.most_similar(positive=['user_friendly'], topn=10))\n",
    "# print(model.wv.most_similar(positive=['great_tool'], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "['qualtrics', 'use', 'support', 'survey', 'great', 'service']\n",
      "['easy_use', 'customer_service', 'software', 'also', 'ease_use', 'easy']\n",
      "['intuitive', 'features', 'quality', 'tool', 'customer', 'platform', 'visceral', 'nonrational']\n",
      "['functionality', 'data', 'think', 'product', 'like', 'good']\n",
      "['robust', 'work', 'still', 'time', 'full-bodied', 'racy', 'rich']\n"
     ]
    }
   ],
   "source": [
    "print(len(clusters))\n",
    "print(clusters[0])\n",
    "print(clusters[1])\n",
    "print(clusters[2])\n",
    "print(clusters[3])\n",
    "print(clusters[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive \"Adjective\" Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "descriptions = []\n",
    "\n",
    "# this is a bigram filter to remove bigrams with words in the blacklist\n",
    "def create_myfilter(parent):\n",
    "    def bigram_filter(w1, w2):\n",
    "        if w1 == parent:\n",
    "            return w2 in blacklist\n",
    "        if w2 == parent:\n",
    "            return w1 in blacklist\n",
    "    return bigram_filter\n",
    "\n",
    "# for every main/noun cluster....\n",
    "for i, cluster in enumerate(clusters):\n",
    "    # make a copy of the text, and replace \n",
    "    # all cluster words with the parent_word of that cluster\n",
    "    parent_word = cluster[0]\n",
    "    similar_words = cluster[1:]\n",
    "    words_copy = [parent_word if w in similar_words else w for w in combined_words]\n",
    "    \n",
    "    # find top 10 bigrams containing the parent_word\n",
    "    # and NOT contains blacklist words\n",
    "    finder = BigramCollocationFinder.from_words(words_copy, window_size=5)\n",
    "    parent_filter = lambda *w: parent_word not in w   \n",
    "    blacklist_filter = create_myfilter(parent_word)\n",
    "    finder.apply_ngram_filter(parent_filter)      # bigram must contain parent_word\n",
    "    finder.apply_ngram_filter(blacklist_filter)   # bigram does not contain blacklist words\n",
    "    finder.apply_freq_filter(3)                   # bigram occurs at least 3 times\n",
    "    best_bigrams = finder.nbest(bigram_measures.likelihood_ratio, 10)\n",
    "    adj = []\n",
    "    list(adj.extend(row) for row in best_bigrams)\n",
    "    l = [w for w in adj if w != parent_word]\n",
    "    descriptions.append(adj)\n",
    "\n",
    "for i,cluster in enumerate(clusters):\n",
    "    descriptions[i] = [w for w in descriptions[i] if w != cluster[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tech', 'monkey', 'builder', 'affordable', 'building', 'analytic', 'enables', 'replying', 'quick', 'allows']\n",
      "['learn', 'saves', 'featured', 'little', 'everything', 'build', 'question_types', 'affordable', 'constantly', 'beginner']\n",
      "['top', 'high', 'curve', 'develop', 'advanced', 'needed', 'overall', 'adding', 'application', 'generally']\n"
     ]
    }
   ],
   "source": [
    "print(descriptions[0])\n",
    "print(descriptions[1])\n",
    "print(descriptions[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clustfilter(clust):\n",
    "    def bigram_filter(w1, w2):\n",
    "        return w2 not in clust and w1 not in clust\n",
    "    return bigram_filter\n",
    "\n",
    "topics = []\n",
    "for i in range(len(clusters)):\n",
    "    pool = clusters[i] + descriptions[i]\n",
    "    finder = BigramCollocationFinder.from_words(combined_words, window_size=2)\n",
    "    clust_filter = create_clustfilter(pool)  \n",
    "    topic_filter = lambda w1, w2: (w1, w2) in set(topics)\n",
    "    finder.apply_ngram_filter(clust_filter)\n",
    "    finder.apply_ngram_filter(topic_filter)\n",
    "    topics.append(finder.nbest(bigram_measures.likelihood_ratio, 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('easy', 'use'),\n",
       " ('easy', 'learn'),\n",
       " ('customer', 'service'),\n",
       " ('quality', 'product'),\n",
       " ('user', 'friendly'),\n",
       " ('much', 'easier'),\n",
       " ('ease', 'use'),\n",
       " ('tech', 'support'),\n",
       " ('question', 'types'),\n",
       " ('need', 'help')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_topics = []\n",
    "for i,topic in enumerate(topics):\n",
    "    topicName = \" \".join(topic)\n",
    "    clust = clusters[i]\n",
    "    adj = descriptions[i]\n",
    "    queryParams = [topicName, clust, adj[1:]]\n",
    "    final_topics.append(queryParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['easy use',\n",
       "  ['qualtrics', 'use', 'support', 'survey', 'great', 'service'],\n",
       "  ['monkey',\n",
       "   'builder',\n",
       "   'affordable',\n",
       "   'building',\n",
       "   'analytic',\n",
       "   'enables',\n",
       "   'replying',\n",
       "   'quick',\n",
       "   'allows']],\n",
       " ['easy learn',\n",
       "  ['easy_use', 'customer_service', 'software', 'also', 'ease_use', 'easy'],\n",
       "  ['saves',\n",
       "   'featured',\n",
       "   'little',\n",
       "   'everything',\n",
       "   'build',\n",
       "   'question_types',\n",
       "   'affordable',\n",
       "   'constantly',\n",
       "   'beginner']]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_topics[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open('topicsKaite.txt', 'w')\n",
    "\n",
    "for topic in final_topics[:10]:\n",
    "    outfile.write(\"%s\\n\" % topic[0])\n",
    "    outfile.write(\"%s\\n\" % topic[1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
